{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets accelerate nvidia-ml-py3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsdqpEpFxGRc",
        "outputId": "8cad41c1-2964-45e0-f906-d72f18cd9e76"
      },
      "id": "rsdqpEpFxGRc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.7.1-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.7/dist-packages (7.352.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.50)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.3.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.11.0+cu113)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: click==8.0 in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (8.0.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "271779c6",
      "metadata": {
        "id": "271779c6"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "\n",
        "# import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import Trainer, AutoTokenizer, AutoModelForCausalLM, TrainingArguments, default_data_collator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "SPvW4xCOxYJo"
      },
      "id": "SPvW4xCOxYJo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pynvml import *\n",
        "\n",
        "\n",
        "def print_gpu_utilization():\n",
        "    nvmlInit()\n",
        "    handle = nvmlDeviceGetHandleByIndex(0)\n",
        "    info = nvmlDeviceGetMemoryInfo(handle)\n",
        "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
        "\n",
        "\n",
        "def print_summary(result):\n",
        "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
        "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
        "    print_gpu_utilization()"
      ],
      "metadata": {
        "id": "SQQwZm7MyXEW"
      },
      "id": "SQQwZm7MyXEW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81c4036e",
      "metadata": {
        "id": "81c4036e"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = Path(\"./checkpoints/model-test\")\n",
        "save_model_dir = Path(\"./models/model-test\")\n",
        "eval_result_dir = Path(\"./eval\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a795fc6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a795fc6",
        "outputId": "5ae91c58-09fb-4869-ad93-5fab5a1a1fa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================================================================================\n",
            "device: cpu\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(\"=\"*100)\n",
        "print(f\"device: {device}\")\n",
        "print(\"=\"*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c233f6e",
      "metadata": {
        "id": "8c233f6e"
      },
      "outputs": [],
      "source": [
        "replace_dict = {'and': ['&', \"'n\"], '': ['%', ',', '.', '#', '[', ']', '!', '?']}\n",
        "\n",
        "def clean_instruction(instruction):\n",
        "    instruction = instruction.lower()\n",
        "    for rep, char_list in replace_dict.items():\n",
        "        for c_ in char_list:\n",
        "            if c_ in instruction:\n",
        "                instruction = instruction.replace(c_, rep)\n",
        "        instruction = instruction.strip()\n",
        "\n",
        "    # remove sentences starting with \"1.\", \"2.\", ... from the targets\n",
        "    if len(instruction) > 0 and instruction[0].isdigit():\n",
        "        instruction = ''\n",
        "    return instruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddbdefad",
      "metadata": {
        "id": "ddbdefad"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(row):\n",
        "    title = row[\"title\"]\n",
        "    ingrs = ingrs_next.join(row[\"base_ingrs\"].split(\";\"))\n",
        "    instructions = f\"{instr_next}\".join([clean_instruction(inst) for inst in row[\"instructions\"].split(\";\")])\n",
        "\n",
        "    seq = f\"{bos_token}{ingrs_start}{ingrs}{ingrs_end}{instr_start}{instructions}{instr_end}{title_start}{title}{title_end}{eos_token}\"\n",
        "    tkns = tokenizer(seq, truncation=True)\n",
        "    tkns[\"labels\"] = tkns[\"input_ids\"].copy()\n",
        "\n",
        "    return tkns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85a3aca8",
      "metadata": {
        "id": "85a3aca8"
      },
      "outputs": [],
      "source": [
        "block_size = 128\n",
        "\n",
        "def pad_group_texts(examples):\n",
        "    # Concatenate all texts.\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "\n",
        "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
        "        # customize this part to your needs.\n",
        "\n",
        "    full_length = (total_length // block_size) * block_size\n",
        "    pad_length = block_size - (total_length - full_length)\n",
        "\n",
        "    padded_seq = {\n",
        "        'input_ids': concatenated_examples['input_ids'] + ([tokenizer.convert_tokens_to_ids('<PAD>')] * pad_length),\n",
        "        'attention_mask': concatenated_examples['attention_mask'] + ([0] * pad_length)\n",
        "    }\n",
        "\n",
        "    # Split by chunks of max_len.\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length + pad_length, block_size)]\n",
        "        for k, t in padded_seq.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2d66238",
      "metadata": {
        "id": "d2d66238"
      },
      "outputs": [],
      "source": [
        "bos_token = \"<BOS>\"\n",
        "eos_token = \"<EOS>\"\n",
        "pad_token = \"<PAD>\"\n",
        "\n",
        "ingrs_start = \"<INGRS_START>\"\n",
        "ingrs_end = \"<INGRS_END>\"\n",
        "ingrs_next = \"<INGRS_NEXT>\"\n",
        "instr_start = \"<INSTR_START>\"\n",
        "instr_end = \"<INSTR_END>\"\n",
        "instr_next = \"<INSTR_NEXT>\"\n",
        "title_start = \"<TITLE_START>\"\n",
        "title_end = \"<TITLE_END>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "460b9ada",
      "metadata": {
        "id": "460b9ada"
      },
      "outputs": [],
      "source": [
        "special_tokens = [ingrs_start, ingrs_end, ingrs_next, instr_start, instr_end, instr_next, title_start, title_end]\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"distilgpt2\",\n",
        "    additional_special_tokens=special_tokens,\n",
        "    bos_token=bos_token,\n",
        "    eos_token=eos_token,\n",
        "    pad_token=pad_token,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c797a56",
      "metadata": {
        "id": "8c797a56"
      },
      "outputs": [],
      "source": [
        "# test_df = pd.read_csv(\"../dataset/new_merged/test_merged.csv\")\n",
        "train_df = pd.read_csv(\"../dataset/new_merged/train_merged.csv\")\n",
        "val_df = pd.read_csv(\"../dataset/new_merged/val_merged.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4713fd04",
      "metadata": {
        "id": "4713fd04"
      },
      "outputs": [],
      "source": [
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "train_dataset = train_dataset.shuffle()\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_function, remove_columns=list(train_df.columns), num_proc=8)\n",
        "\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "val_dataset = val_dataset.shuffle()\n",
        "tokenized_val_dataset = val_dataset.map(tokenize_function, remove_columns=list(val_df.columns), num_proc=8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f72ccaa4",
      "metadata": {
        "id": "f72ccaa4"
      },
      "outputs": [],
      "source": [
        "lm_train_datasets = tokenized_train_dataset.map(\n",
        "#     group_texts,\n",
        "    pad_group_texts,\n",
        "    batched=True,\n",
        "    batch_size=1000,\n",
        "    num_proc=8,\n",
        ")\n",
        "lm_train_datasets.set_format(\"pt\")\n",
        "\n",
        "pickle.dump(lm_train_datasets, open(\"./lm_train_datasets.pkl\", \"wb\"))\n",
        "\n",
        "lm_val_datasets = tokenized_val_dataset.map(\n",
        "#     group_texts,\n",
        "    pad_group_texts,\n",
        "    batched=True,\n",
        "    batch_size=1000,\n",
        "    num_proc=8,\n",
        ")\n",
        "lm_val_datasets.set_format(\"pt\")\n",
        "\n",
        "pickle.dump(lm_val_datasets, open(\"./lm_val_datasets.pkl\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b4bf989",
      "metadata": {
        "id": "1b4bf989"
      },
      "outputs": [],
      "source": [
        "# lm_train_datasets = pickle.load(open(\"./lm_train_datasets.pkl\", \"rb\"))\n",
        "# lm_val_datasets = pickle.load(open(\"./lm_val_datasets.pkl\", \"rb\"))\n",
        "\n",
        "print(lm_train_datasets)\n",
        "print(lm_val_datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d85c070f",
      "metadata": {
        "id": "d85c070f"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "model.to(device)\n",
        "data_collator = default_data_collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8263f773",
      "metadata": {
        "id": "8263f773"
      },
      "outputs": [],
      "source": [
        "default_args = {\n",
        "\t\"output_dir\": checkpoint_dir,\n",
        "\t\"learning_rate\": 2e-5,\n",
        "\t\"weight_deacy\": 0.01,\n",
        "\t\"save_strategy\": \"epoch\",\n",
        "\t\"logging_strategy\": \"steps\",\n",
        "\t\"logging_steps\": 100,\n",
        "    \"num_train_epochs\": 10\n",
        "}\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "\tper_device_train_batch_size = 4,\n",
        "\tper_device_eval_batch_size = 4,\n",
        "\tgradient_accumulation_steps = 32\n",
        "\t**default_args\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fa96ae5",
      "metadata": {
        "id": "2fa96ae5"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_arguments,\n",
        "    train_dataset=lm_train_datasets,\n",
        "    eval_dataset=lm_val_datasets,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55f4bf68",
      "metadata": {
        "id": "55f4bf68"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "# trainer.train(resume_from_checkpoint=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fe2c015",
      "metadata": {
        "id": "4fe2c015"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(save_model_dir)\n",
        "print(f\"model saved at {save_model_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcef4153",
      "metadata": {
        "id": "fcef4153"
      },
      "outputs": [],
      "source": [
        "eval_results = trainer.evaluate()\n",
        "\n",
        "result_path = eval_result_dir / \"eval_result.json\"\n",
        "json.dump(eval_results, open(result_path, \"w\"))\n",
        "\n",
        "print(eval_results)\n",
        "\n",
        "# print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "8RrGlsN_xizZ"
      },
      "id": "8RrGlsN_xizZ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "bct-major-project",
      "language": "python",
      "name": "bct-major-project"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}